{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import re # regex\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>downs</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>body</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f1lq</td>\n",
       "      <td>t1_cqug92b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug92b</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hogsucker</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqu4t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34exjb</td>\n",
       "      <td>t1_cqug96h</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug96h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flal4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqudz0p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f10p</td>\n",
       "      <td>t1_cqug9tk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug9tk</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HitachinoBia</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqufsip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "      <td>t1_cquga1l</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1l</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34e7eo</td>\n",
       "      <td>t1_cquga1v</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1v</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cultiststeve</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqudxkr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc   ups subreddit_id    link_id        name  score_hidden  \\\n",
       "0  1430438402 -11.0     t5_2qh3l  t3_34f1lq  t1_cqug92b           0.0   \n",
       "1  1430438407   1.0     t5_2qh3l  t3_34exjb  t1_cqug96h           0.0   \n",
       "2  1430438439   4.0     t5_2qh3l  t3_34f10p  t1_cqug9tk           0.0   \n",
       "3  1430438448   0.0     t5_2qh3l  t3_34cvvg  t1_cquga1l           0.0   \n",
       "4  1430438449 -10.0     t5_2qh3l  t3_34e7eo  t1_cquga1v           0.0   \n",
       "\n",
       "   author_flair_css_class  author_flair_text subreddit       id  ...  downs  \\\n",
       "0                     NaN                NaN      news  cqug92b  ...    0.0   \n",
       "1                     NaN                NaN      news  cqug96h  ...    0.0   \n",
       "2                     NaN                NaN      news  cqug9tk  ...    0.0   \n",
       "3                     NaN                NaN      news  cquga1l  ...    0.0   \n",
       "4                     NaN                NaN      news  cquga1v  ...    0.0   \n",
       "\n",
       "   archived        author  score  retrieved_on  \\\n",
       "0       0.0     hogsucker  -11.0  1.432703e+09   \n",
       "1       0.0         flal4    1.0  1.432703e+09   \n",
       "2       0.0  HitachinoBia    4.0  1.432703e+09   \n",
       "3       0.0     [deleted]    0.0  1.432703e+09   \n",
       "4       0.0  Cultiststeve  -10.0  1.432703e+09   \n",
       "\n",
       "                                                body  distinguished edited  \\\n",
       "0  1-She got to be a bigwig at Google by sleeping...            NaN    0.0   \n",
       "1  For those about to lynch this guy [here](http:...            NaN    0.0   \n",
       "2  It feels like black people are the most racist...            NaN    0.0   \n",
       "3                                          [deleted]            NaN    0.0   \n",
       "4  Its because otherwise thats all that would app...            NaN    0.0   \n",
       "\n",
       "  controversiality   parent_id  \n",
       "0              0.0  t1_cqu4t11  \n",
       "1              1.0  t1_cqudz0p  \n",
       "2              1.0  t1_cqufsip  \n",
       "3              0.0   t3_34cvvg  \n",
       "4              0.0  t1_cqudxkr  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('../datasets/news-20k-comments.csv')\n",
    "politics_df = pd.read_csv(\"../datasets/politics-20k-comments.csv\")\n",
    "politicaldiscussion_df = pd.read_csv('../datasets/politicaldiscussion-20k-comments.csv')\n",
    "\n",
    "raw_df = pd.concat([news_df, politics_df, politicaldiscussion_df]) # merge all three subreddit datas\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = raw_df.filter(['created_utc', 'subreddit', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out deleted comments\n",
    "comments = all_comments[all_comments['body'] != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-993bcaba2f48>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['body'] = comments['body'].astype('str')\n",
      "<ipython-input-5-993bcaba2f48>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1 she got to be a bigwig at google by sleeping...\n",
       "1    for those about to lynch this guy here is a sh...\n",
       "2    it feels like black people are the most racist...\n",
       "4    its because otherwise thats all that would app...\n",
       "5    please go to facebook and comment and post on ...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean out any urls and and brackets, parenthesis and hyphens, leaving only alphanumeric words\n",
    "url_regex = r\"([--:\\w?@%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?\"\n",
    "special_character_regex = r\"[\\\"'()[\\]]\"\n",
    "\n",
    "comments['body'] = comments['body'].astype('str')\n",
    "\n",
    "#remove urls, special characters, and replace hyphens with a space\n",
    "comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n",
    "comments['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d6433c9309aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)\n"
     ]
    }
   ],
   "source": [
    "comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55567 total comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>news</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>1 she got to be a bigwig at google by sleeping...</td>\n",
       "      <td>[she, got, to, be, a, bigwig, at, google, by, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>news</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>for those about to lynch this guy here is a sh...</td>\n",
       "      <td>[for, those, about, to, lynch, this, guy, here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>news</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>it feels like black people are the most racist...</td>\n",
       "      <td>[it, feels, like, black, people, are, the, mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>news</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>its because otherwise thats all that would app...</td>\n",
       "      <td>[its, because, otherwise, thats, all, that, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438456</td>\n",
       "      <td>news</td>\n",
       "      <td>Please go to Facebook and comment and post on ...</td>\n",
       "      <td>please go to facebook and comment and post on ...</td>\n",
       "      <td>[please, go, to, facebook, and, comment, and, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc subreddit                                               body  \\\n",
       "0  1430438402      news  1-She got to be a bigwig at Google by sleeping...   \n",
       "1  1430438407      news  For those about to lynch this guy [here](http:...   \n",
       "2  1430438439      news  It feels like black people are the most racist...   \n",
       "3  1430438449      news  Its because otherwise thats all that would app...   \n",
       "4  1430438456      news  Please go to Facebook and comment and post on ...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  1 she got to be a bigwig at google by sleeping...   \n",
       "1  for those about to lynch this guy here is a sh...   \n",
       "2  it feels like black people are the most racist...   \n",
       "3  its because otherwise thats all that would app...   \n",
       "4  please go to facebook and comment and post on ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [she, got, to, be, a, bigwig, at, google, by, ...  \n",
       "1  [for, those, about, to, lynch, this, guy, here...  \n",
       "2  [it, feels, like, black, people, are, the, mos...  \n",
       "3  [its, because, otherwise, thats, all, that, wo...  \n",
       "4  [please, go, to, facebook, and, comment, and, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = comments.reset_index(drop=True)\n",
    "print(len(comments), \"total comments\")\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55567"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for comment in comments['clean'][:10000]:\n",
    "    all_text += \"<\" + comment + \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max id: 102 \n",
      "dataset_size: 2129260\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "print(\"max id:\", max_id, \"\\ndataset_size:\", dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([all_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, None, 128)         89088     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 102)         13158     \n",
      "=================================================================\n",
      "Total params: 201,318\n",
      "Trainable params: 201,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59883/59883 [==============================] - 841s 14ms/step - loss: 1.6509\n",
      "Epoch 2/10\n",
      "59883/59883 [==============================] - 845s 14ms/step - loss: 1.5789\n",
      "Epoch 3/10\n",
      "59883/59883 [==============================] - 845s 14ms/step - loss: 1.5641\n",
      "Epoch 4/10\n",
      "59883/59883 [==============================] - 841s 14ms/step - loss: 1.5563\n",
      "Epoch 5/10\n",
      "59883/59883 [==============================] - 848s 14ms/step - loss: 1.5515\n",
      "Epoch 6/10\n",
      "59883/59883 [==============================] - 845s 14ms/step - loss: 1.5475\n",
      "Epoch 7/10\n",
      "59883/59883 [==============================] - 847s 14ms/step - loss: 1.5448\n",
      "Epoch 8/10\n",
      "59883/59883 [==============================] - 849s 14ms/step - loss: 1.5425\n",
      "Epoch 9/10\n",
      "59883/59883 [==============================] - 853s 14ms/step - loss: 1.5403\n",
      "Epoch 10/10\n",
      "59883/59883 [==============================] - 846s 14ms/step - loss: 1.5386\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fully-trained-with-activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"peopl\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    Y_pred = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled = tf.math.log(Y_pred)/temperature\n",
    "    char_id = tf.random.categorical(rescaled, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n=50, temperature=1):\n",
    "    for _ in range(n):\n",
    "        text += generate_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing the results to the police state of the police statement of the police state of the same thing abo\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"i\", n=100, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = int(len(comments)*0.8) #80/20 train/test split\n",
    "test = comments[boundary:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response to 1. Baseline Model Performance Metrics (Part 1)\n",
    "We've chosen two metrics to evaluate our model's performance on. The first is arguably the most simple, but it is precision plain and simple. We can define precision in our context of generated sentences to be if a token is present in the test vocabulary, and an imprecision is when it's not. This is a good heuristic for a baseline model, as when we improve our model we expect our precision to improve to a certain plateau, as all words in the test set may not occur in the train set and vica-versa.\n",
    "\n",
    "A precision metric is useful for our application because we seek to generate comments similar to other reddit comments, and therefore seek to use similar words as a normal reddit comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "test_vocab = nltk.lm.Vocabulary([word for sent in test['tokens'] for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "evaluation_sentences = []\n",
    "sample_starts = [\"a\", \"the\", \"well\", \"i\", \"possibly\", \"this\"]\n",
    "number_to_generate = 10\n",
    "for i in range(number_to_generate):\n",
    "    start_token = random.choice(sample_starts)\n",
    "    evaluation_sentences.append(generate(start_token, n=100, temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and the police so they were the same thing about the same thing about the same force they will be a b',\n",
       " 'the police are in a thread with a protest, the police seems like they will stop for for a protest, they',\n",
       " 'they will already have the police so they will pull over to the same thing about the same thing about t',\n",
       " 'ing the state of the same thing about the same thing about the same thing about the same of the polic',\n",
       " 'possibly shows and the other than they would be really search something about the same who was stopping and ',\n",
       " 'well it to stopped and the police stopped the same thing about the cops they will already trained to see',\n",
       " 'well over the cops with a baltimore police would be stopping and the same thing about the same force the',\n",
       " 'the police so they will already statement and the other police are something.  i think they would be a ',\n",
       " 'this thing and it was the same thing about the same thing about the police so they will use them and the',\n",
       " 'in a thread with police department was really got the same work is the only thing about the same of t']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating precision\n",
    "generated_tokens = [token for sent in evaluation_sentences for token in sent.split()]\n",
    "\n",
    "true_positive = -10 #we know that 10 of these generated words are already in test_vocab\n",
    "false_positive = 0\n",
    "for word in generated_tokens:\n",
    "    if word in test_vocab:\n",
    "        true_positive += 1\n",
    "    else:\n",
    "        false_positive += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our precision is 0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(\"Our precision is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on Precision Metric\n",
    "\n",
    "We evaluate our precision not on the entire dataset, but only on the last 20% of our dataset (the test partition). We've sorted our data chronologically, so the newest comments are used in the test dataset. This makes virtually no difference for our evaluation as it's only about 2 days worth of comments on reddit.\n",
    "\n",
    "We achieve about 80% to 89% precision. This is really good, but we see that our evaluation sentences are pretty short. Precision doesn't measure fluent mimicry, just if we're hitting the right word tokens, and we are. More epochs and changing the RNN's hyper-parameters would lead to a better position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response to 1. Baseline Model Performance Metrics (Part 2)\n",
    "The second metric we've chosen is perplexity. We want a low perplexity, as that indicates a more cohesive model. Lower perplexity means we have a good model for predicting (or generating, really) text. Luckily for us, Tensorflow / Keras provides a neat little history object we can use to see how the loss function returns. Our loss function was `\"sparse_categorical_crossentropy\"`, and this is fortunate because if we take `e^loss` then we have our perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "[latest_loss] = history.history['loss'][-1:]\n",
    "perplexity = np.e ** latest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for our model:  4.658118127425183\n",
      "Perplexity of our test vocab:  9.980402296304254\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for our model: \", perplexity)\n",
    "print(\"Perplexity of our test vocab: \", np.log(len(test_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on Perpleixty Metric\n",
    "\n",
    "We see that our perplexity of the model is 11.67. This is not fantastic, as the perplexity of the test vocabulary is 9.9. We want the perplexity of our model to be lower, since that would mean a less \"confused\" model and one that's more \"confident\" in it's predictions (generations). We want a model that is closer to that 9.9 perplexity as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion on the Model\n",
    "\n",
    "Recurrent Neural Networks are inherently suited for natural language generation due to their usage of temporal context, which is essential in an application where we are generating (more specifically, predicting) characters. Concretely, we can generate characters based on the previous characters.\n",
    "\n",
    "\n",
    "Our baseline model is very simple, and definitely could use work in crafting it to be better.\n",
    "\n",
    "```\n",
    "Model: \"sequential_3\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "gru_4 (GRU)                  (None, None, 128)         81024     \n",
    "_________________________________________________________________\n",
    "gru_5 (GRU)                  (None, None, 128)         99072     \n",
    "_________________________________________________________________\n",
    "time_distributed_3 (TimeDist (None, None, 81)          10449     \n",
    "=================================================================\n",
    "Total params: 190,545\n",
    "Trainable params: 190,545\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "The first two layers are GRU layers with 128 units each. We have a dropout rate of 20%, which helps with making sure no one neuron is overly relied upon. The output layer is interesting, as it's a time distributed dense layer that has output neurons that hold a bijective relationship with the elements of our vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
