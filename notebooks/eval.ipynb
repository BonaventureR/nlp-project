{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import re # regex\n",
    "import matplotlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "news_df = pd.read_csv('../datasets/news-20k-comments.csv')\n",
    "politics_df = pd.read_csv(\"../datasets/politics-20k-comments.csv\")\n",
    "politicaldiscussion_df = pd.read_csv('../datasets/politicaldiscussion-20k-comments.csv')\n",
    "\n",
    "news_df = news_df.filter(['created_utc', 'body'])\n",
    "politics_df = politics_df.filter(['created_utc', 'body'])\n",
    "politicaldiscussion_df = politicaldiscussion_df.filter(['created_utc', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out deleted comments\n",
    "news_df = news_df[news_df['body'] != \"[deleted]\"]\n",
    "politics_df = politics_df[politics_df['body'] != \"[deleted]\"]\n",
    "politicaldiscussion_df = politicaldiscussion_df[politicaldiscussion_df['body'] != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out any urls and and brackets, parenthesis and hyphens, leaving only alphanumeric words\n",
    "url_regex = r\"([--:\\w?@%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?\"\n",
    "special_character_regex = r\"[\\\"'()[\\]]\"\n",
    "\n",
    "news_df['body'] = news_df['body'].astype('str')\n",
    "politics_df['body'] = politics_df['body'].astype('str')\n",
    "politicaldiscussion_df['body'] = politicaldiscussion_df['body'].astype('str')\n",
    "\n",
    "#remove urls, special characters, and replace hyphens with a space\n",
    "news_df['clean'] = news_df['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n",
    "politics_df['clean'] = politics_df['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n",
    "politicaldiscussion_df['clean'] = politicaldiscussion_df['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "news_df['tokens'] = news_df['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)\n",
    "politics_df['tokens'] = politics_df['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)\n",
    "politicaldiscussion_df['tokens'] = politicaldiscussion_df['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17536 total comments from r/news\n",
      "18857 total comments from r/politics\n",
      "19174 total comments from r/PoliticalDiscussion\n"
     ]
    }
   ],
   "source": [
    "news_df = news_df.reset_index(drop=True)\n",
    "politics_df = politics_df.reset_index(drop=True)\n",
    "politicaldiscussion_df = politicaldiscussion_df.reset_index(drop=True)\n",
    "print(len(news_df), \"total comments from r/news\")\n",
    "print(len(politics_df), \"total comments from r/politics\")\n",
    "print(len(politicaldiscussion_df), \"total comments from r/PoliticalDiscussion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test samples for evals\n",
    "# since we trained using 80% of the total comments, we test on 20% of them.\n",
    "\n",
    "import nltk\n",
    "\n",
    "boundary = int(len(news_df)*0.8) #80/20 train/test split\n",
    "news_df_test = news_df[boundary:]\n",
    "\n",
    "boundary = int(len(politics_df)*0.8) #80/20 train/test split\n",
    "politics_df_test = politics_df[boundary:]\n",
    "\n",
    "boundary = int(len(politicaldiscussion_df)*0.8) #80/20 train/test split\n",
    "politicaldiscussion_df_test = politicaldiscussion_df[boundary:]\n",
    "\n",
    "news_test_vocab = nltk.lm.Vocabulary([word for sent in news_df_test['tokens'] for word in sent])\n",
    "politics_test_vocab = nltk.lm.Vocabulary([word for sent in politics_df_test['tokens'] for word in sent])\n",
    "politicaldiscussion_test_vocab = nltk.lm.Vocabulary([word for sent in politicaldiscussion_df_test['tokens'] for word in sent])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, len(tokenizer.word_index))\n",
    "\n",
    "def generate_char(model, tokenizer, text, temperature=1):\n",
    "    X_new = preprocess(tokenizer, [text])\n",
    "    Y_pred = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled = tf.math.log(Y_pred)/temperature\n",
    "    char_id = tf.random.categorical(rescaled, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def generate(model, tokenizer, text, n=50, temperature=1):\n",
    "    for _ in range(n):\n",
    "        text += generate_char(model, tokenizer, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def random_char():\n",
    "    return random.choice(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "\n",
    "random_char()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make tokenizers for each of the models\n",
    "news_all_text = \"\"\n",
    "for comment in news_df['clean']:\n",
    "    news_all_text += \"<\" + comment + \">\"\n",
    "    \n",
    "politics_all_text = \"\"\n",
    "for comment in politics_df['clean']:\n",
    "    politics_all_text += \"<\" + comment + \">\"\n",
    "    \n",
    "political_discussion_all_text = \"\"\n",
    "for comment in politicaldiscussion_df['clean']:\n",
    "    political_discussion_all_text += \"<\" + comment + \">\"\n",
    "    \n",
    "news_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "news_tokenizer.fit_on_texts(news_all_text)\n",
    "\n",
    "politics_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "politics_tokenizer.fit_on_texts(politics_all_text)\n",
    "\n",
    "political_discussion_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "political_discussion_tokenizer.fit_on_texts(political_discussion_all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "news_model = keras.models.load_model('saved_models/NEWS_RNN_2_GRU_128_SOFTMAX_03_DROPOUT_EARLY_STOPPING', compile=False)\n",
    "politics_model = keras.models.load_model('saved_models/POLITICS_RNN_2_GRU_128_SOFTMAX_03_DROPOUT_EARLY_STOPPING', compile=False)\n",
    "political_discussion_model = keras.models.load_model('saved_models/POLDIS_RNN_2_GRU_128_SOFTMAX_03_DROPOUT_EARLY_STOPPING/', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake sentence from r/news:\n",
      " and the police are the rich and the cops were referring to the same than the police are the way that  \n",
      "\n",
      "Fake sentence from r/politics:\n",
      " questions that is the people with a conservative with the worst commanity of the same and the america \n",
      "\n",
      "Fake sentence from r/PoliticalDiscussion:\n",
      " ke the market party in the country who want to be a control of the problem is that the result of the  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fake sentence from r/news:\\n\", generate(news_model, news_tokenizer, random_char(), n=100, temperature=0.2),\"\\n\")\n",
    "print(\"Fake sentence from r/politics:\\n\", generate(politics_model, politics_tokenizer, random_char(), n=100, temperature=0.2),\"\\n\")\n",
    "print(\"Fake sentence from r/PoliticalDiscussion:\\n\", generate(political_discussion_model, political_discussion_tokenizer, random_char(), n=100, temperature=0.2),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities of test vocabularies\n",
      "==============================\n",
      "r/news: 9.158309963881798\n",
      "r/politics: 9.319374133000306\n",
      "r/PoliticalDiscussion: 9.49363784611014\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexities of test vocabularies\")\n",
    "print(\"==============================\")\n",
    "print(\"r/news:\", np.log(len(news_test_vocab)))\n",
    "print(\"r/politics:\", np.log(len(politics_test_vocab)))\n",
    "print(\"r/PoliticalDiscussion:\", np.log(len(politicaldiscussion_test_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities of models\n",
      "r/news: 5.864985442469667\n",
      "r/politics: 4.437095519003664\n",
      "r/PoliticalDiscussion: 5.528961477624003\n"
     ]
    }
   ],
   "source": [
    "#losses from tensorflow logs\n",
    "news_loss = 1.769\n",
    "politics_loss = 1.49\n",
    "political_discussion_loss = 1.71\n",
    "print(\"Perplexities of models\")\n",
    "(\"==============================\")\n",
    "print(\"r/news:\", np.e ** news_loss)\n",
    "print(\"r/politics:\", np.e ** politics_loss)\n",
    "print(\"r/PoliticalDiscussion:\", np.e ** political_discussion_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
