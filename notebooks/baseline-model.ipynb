{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import re # regex\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>downs</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>body</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f1lq</td>\n",
       "      <td>t1_cqug92b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug92b</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hogsucker</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqu4t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34exjb</td>\n",
       "      <td>t1_cqug96h</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug96h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flal4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqudz0p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f10p</td>\n",
       "      <td>t1_cqug9tk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug9tk</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HitachinoBia</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqufsip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "      <td>t1_cquga1l</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1l</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34e7eo</td>\n",
       "      <td>t1_cquga1v</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1v</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cultiststeve</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqudxkr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc   ups subreddit_id    link_id        name  score_hidden  \\\n",
       "0  1430438402 -11.0     t5_2qh3l  t3_34f1lq  t1_cqug92b           0.0   \n",
       "1  1430438407   1.0     t5_2qh3l  t3_34exjb  t1_cqug96h           0.0   \n",
       "2  1430438439   4.0     t5_2qh3l  t3_34f10p  t1_cqug9tk           0.0   \n",
       "3  1430438448   0.0     t5_2qh3l  t3_34cvvg  t1_cquga1l           0.0   \n",
       "4  1430438449 -10.0     t5_2qh3l  t3_34e7eo  t1_cquga1v           0.0   \n",
       "\n",
       "   author_flair_css_class  author_flair_text subreddit       id  ...  downs  \\\n",
       "0                     NaN                NaN      news  cqug92b  ...    0.0   \n",
       "1                     NaN                NaN      news  cqug96h  ...    0.0   \n",
       "2                     NaN                NaN      news  cqug9tk  ...    0.0   \n",
       "3                     NaN                NaN      news  cquga1l  ...    0.0   \n",
       "4                     NaN                NaN      news  cquga1v  ...    0.0   \n",
       "\n",
       "   archived        author  score  retrieved_on  \\\n",
       "0       0.0     hogsucker  -11.0  1.432703e+09   \n",
       "1       0.0         flal4    1.0  1.432703e+09   \n",
       "2       0.0  HitachinoBia    4.0  1.432703e+09   \n",
       "3       0.0     [deleted]    0.0  1.432703e+09   \n",
       "4       0.0  Cultiststeve  -10.0  1.432703e+09   \n",
       "\n",
       "                                                body  distinguished edited  \\\n",
       "0  1-She got to be a bigwig at Google by sleeping...            NaN    0.0   \n",
       "1  For those about to lynch this guy [here](http:...            NaN    0.0   \n",
       "2  It feels like black people are the most racist...            NaN    0.0   \n",
       "3                                          [deleted]            NaN    0.0   \n",
       "4  Its because otherwise thats all that would app...            NaN    0.0   \n",
       "\n",
       "  controversiality   parent_id  \n",
       "0              0.0  t1_cqu4t11  \n",
       "1              1.0  t1_cqudz0p  \n",
       "2              1.0  t1_cqufsip  \n",
       "3              0.0   t3_34cvvg  \n",
       "4              0.0  t1_cqudxkr  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('../datasets/news-20k-comments.csv')\n",
    "politics_df = pd.read_csv(\"../datasets/politics-20k-comments.csv\")\n",
    "politicaldiscussion_df = pd.read_csv('../datasets/politicaldiscussion-20k-comments.csv')\n",
    "\n",
    "raw_df = pd.concat([news_df, politics_df, politicaldiscussion_df]) # merge all three subreddit datas\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = raw_df.filter(['created_utc', 'subreddit', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out deleted comments\n",
    "comments = all_comments[all_comments['body'] != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-993bcaba2f48>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['body'] = comments['body'].astype('str')\n",
      "<ipython-input-5-993bcaba2f48>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1 she got to be a bigwig at google by sleeping...\n",
       "1    for those about to lynch this guy here is a sh...\n",
       "2    it feels like black people are the most racist...\n",
       "4    its because otherwise thats all that would app...\n",
       "5    please go to facebook and comment and post on ...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean out any urls and and brackets, parenthesis and hyphens, leaving only alphanumeric words\n",
    "url_regex = r\"([--:\\w?@%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?\"\n",
    "special_character_regex = r\"[\\\"'()[\\]]\"\n",
    "\n",
    "comments['body'] = comments['body'].astype('str')\n",
    "\n",
    "#remove urls, special characters, and replace hyphens with a space\n",
    "comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n",
    "comments['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-d6433c9309aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)\n"
     ]
    }
   ],
   "source": [
    "comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55567 total comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>news</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>1 she got to be a bigwig at google by sleeping...</td>\n",
       "      <td>[she, got, to, be, a, bigwig, at, google, by, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>news</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>for those about to lynch this guy here is a sh...</td>\n",
       "      <td>[for, those, about, to, lynch, this, guy, here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>news</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>it feels like black people are the most racist...</td>\n",
       "      <td>[it, feels, like, black, people, are, the, mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>news</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>its because otherwise thats all that would app...</td>\n",
       "      <td>[its, because, otherwise, thats, all, that, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438456</td>\n",
       "      <td>news</td>\n",
       "      <td>Please go to Facebook and comment and post on ...</td>\n",
       "      <td>please go to facebook and comment and post on ...</td>\n",
       "      <td>[please, go, to, facebook, and, comment, and, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc subreddit                                               body  \\\n",
       "0  1430438402      news  1-She got to be a bigwig at Google by sleeping...   \n",
       "1  1430438407      news  For those about to lynch this guy [here](http:...   \n",
       "2  1430438439      news  It feels like black people are the most racist...   \n",
       "3  1430438449      news  Its because otherwise thats all that would app...   \n",
       "4  1430438456      news  Please go to Facebook and comment and post on ...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  1 she got to be a bigwig at google by sleeping...   \n",
       "1  for those about to lynch this guy here is a sh...   \n",
       "2  it feels like black people are the most racist...   \n",
       "3  its because otherwise thats all that would app...   \n",
       "4  please go to facebook and comment and post on ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [she, got, to, be, a, bigwig, at, google, by, ...  \n",
       "1  [for, those, about, to, lynch, this, guy, here...  \n",
       "2  [it, feels, like, black, people, are, the, mos...  \n",
       "3  [its, because, otherwise, thats, all, that, wo...  \n",
       "4  [please, go, to, facebook, and, comment, and, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = comments.reset_index(drop=True)\n",
    "print(len(comments), \"total comments\")\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55567"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for comment in comments['clean'][:5000]:\n",
    "    all_text += \"<\" + comment + \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max id: 81 \n",
      "dataset_size: 1087981\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "print(\"max id:\", max_id, \"\\ndataset_size:\", dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([all_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, None, 128)         81024     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 81)          10449     \n",
      "=================================================================\n",
      "Total params: 190,545\n",
      "Trainable params: 190,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30597/30597 [==============================] - 410s 13ms/step - loss: 2.5878\n",
      "Epoch 2/10\n",
      "30597/30597 [==============================] - 409s 13ms/step - loss: 2.5491\n",
      "Epoch 3/10\n",
      "30597/30597 [==============================] - 412s 13ms/step - loss: 2.5230\n",
      "Epoch 4/10\n",
      "30597/30597 [==============================] - 409s 13ms/step - loss: 2.5098\n",
      "Epoch 5/10\n",
      "30597/30597 [==============================] - 409s 13ms/step - loss: 2.4784\n",
      "Epoch 6/10\n",
      "30597/30597 [==============================] - 408s 13ms/step - loss: 2.4692\n",
      "Epoch 7/10\n",
      "30597/30597 [==============================] - 408s 13ms/step - loss: 2.5023\n",
      "Epoch 8/10\n",
      "30597/30597 [==============================] - 409s 13ms/step - loss: 2.4948\n",
      "Epoch 9/10\n",
      "30597/30597 [==============================] - 420s 14ms/step - loss: 2.4831\n",
      "Epoch 10/10\n",
      "30597/30597 [==============================] - 415s 14ms/step - loss: 2.4578\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"peopl\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    Y_pred = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled = tf.math.log(Y_pred)/temperature\n",
    "    char_id = tf.random.categorical(rescaled, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n=50, temperature=1):\n",
    "    for _ in range(n):\n",
    "        text += generate_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it i in int at a tant an ci  ane the pei  e an o cere the tar ant c an hat the a perti t in t at the \n"
     ]
    }
   ],
   "source": [
    "print(generate(\"i\", n=100, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = int(len(comments)*0.8) #80/20 train/test split\n",
    "test = comments[boundary:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response to 1. Baseline Model Performance Metrics (Part 1)\n",
    "We've chosen two metrics to evaluate our model's performance on. The first is arguably the most simple, but it is precision plain and simple. We can define precision in our context of generated sentences to be if a token is present in the test vocabulary, and an imprecision is when it's not. This is a good heuristic for a baseline model, as when we improve our model we expect our precision to improve to a certain plateau, as all words in the test set may not occur in the train set and vica-versa.\n",
    "\n",
    "A precision metric is useful for our application because we seek to generate comments similar to other reddit comments, and therefore seek to use similar words as a normal reddit comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "test_vocab = nltk.lm.Vocabulary([word for sent in test['tokens'] for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "evaluation_sentences = []\n",
    "sample_starts = [\"a\", \"the\", \"well\", \"i\", \"possibly\", \"this\"]\n",
    "number_to_generate = 10\n",
    "for i in range(number_to_generate):\n",
    "    start_token = random.choice(sample_starts)\n",
    "    evaluation_sentences.append(generate(start_token, n=100, temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['possibly t e tent an o be hon the rene and i c   an w a   a c   on th sace to the dent he the en the the c  ',\n",
       " 'well an out in ere the ne the the the ten the no tus t en the ant the nest b  an the c ane ance c  to c ',\n",
       " 'the wat the in on th t per at the to the the about th t the the the a c  an the the des the a a c  tect',\n",
       " 'possibly an the c nie b it t at the ton th t it b to m and an a te tant o the lont are an it o  the the a th',\n",
       " 'this the b me an the the tan the tine t the tn to the to c  the tan c  b the c as a c   the a c   ant in',\n",
       " 'well the ment the han acou  e the sace a w nt t it in c   on th thice   on th t at the her to the pent o',\n",
       " 'well a c all ent a to the c oned an the dent a l to in an the in an the tere the pent aire the no c  an ',\n",
       " 'at ance she ant a ti  t tan a c   in t i  er an in t the a to tune th t at o con ane the the ten c  b',\n",
       " 'this an the c on the c pininal a c an to the c in w be the to a to a to a tall a c   on th tuite c  the ',\n",
       " 'possibly an a c tt cond an the in the c  p  tunt t en the the in h a to the ma the hin o ere the c  a r the ']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating precision\n",
    "generated_tokens = [token for sent in evaluation_sentences for token in sent.split()]\n",
    "\n",
    "true_positive = -10 #we know that 10 of these generated words are already in test_vocab\n",
    "false_positive = 0\n",
    "for word in generated_tokens:\n",
    "    if word in test_vocab:\n",
    "        true_positive += 1\n",
    "    else:\n",
    "        false_positive += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our precision is 0.8825503355704698\n"
     ]
    }
   ],
   "source": [
    "precision = true_positive / (true_positive + false_positive)\n",
    "print(\"Our precision is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on Precision Metric\n",
    "\n",
    "We evaluate our precision not on the entire dataset, but only on the last 20% of our dataset (the test partition). We've sorted our data chronologically, so the newest comments are used in the test dataset. This makes virtually no difference for our evaluation as it's only about 2 days worth of comments on reddit.\n",
    "\n",
    "We achieve about 80% to 89% precision. This is really good, but we see that our evaluation sentences are pretty short. Precision doesn't measure fluent mimicry, just if we're hitting the right word tokens, and we are. More epochs and changing the RNN's hyper-parameters would lead to a better position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response to 1. Baseline Model Performance Metrics (Part 2)\n",
    "The second metric we've chosen is perplexity. We want a low perplexity, as that indicates a more cohesive model. Lower perplexity means we have a good model for predicting (or generating, really) text. Luckily for us, Tensorflow / Keras provides a neat little history object we can use to see how the loss function returns. Our loss function was `\"sparse_categorical_crossentropy\"`, and this is fortunate because if we take `e^loss` then we have our perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "[latest_loss] = history.history['loss'][-1:]\n",
    "perplexity = np.e ** latest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for our model:  11.679503125466205\n",
      "Perplexity of our test vocab:  9.980402296304254\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity for our model: \", perplexity)\n",
    "print(\"Perplexity of our test vocab: \", np.log(len(test_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on Perpleixty Metric\n",
    "\n",
    "We see that our perplexity of the model is 11.67. This is not fantastic, as the perplexity of the test vocabulary is 9.9. We want the perplexity of our model to be lower, since that would mean a less \"confused\" model and one that's more \"confident\" in it's predictions (generations). We want a model that is closer to that 9.9 perplexity as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion on the Model\n",
    "\n",
    "Recurrent Neural Networks are inherently suited for natural language generation due to their usage of temporal context, which is essential in an application where we are generating (more specifically, predicting) characters. Concretely, we can generate characters based on the previous characters.\n",
    "\n",
    "\n",
    "Our baseline model is very simple, and definitely could use work in crafting it to be better.\n",
    "\n",
    "```\n",
    "Model: \"sequential_3\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "gru_4 (GRU)                  (None, None, 128)         81024     \n",
    "_________________________________________________________________\n",
    "gru_5 (GRU)                  (None, None, 128)         99072     \n",
    "_________________________________________________________________\n",
    "time_distributed_3 (TimeDist (None, None, 81)          10449     \n",
    "=================================================================\n",
    "Total params: 190,545\n",
    "Trainable params: 190,545\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "The first two layers are GRU layers with 128 units each. We have a dropout rate of 20%, which helps with making sure no one neuron is overly relied upon. The output layer is interesting, as it's a time distributed dense layer that has output neurons that hold a bijective relationship with the elements of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
