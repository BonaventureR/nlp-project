{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import re # regex\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>downs</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>body</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>edited</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f1lq</td>\n",
       "      <td>t1_cqug92b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug92b</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hogsucker</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqu4t11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34exjb</td>\n",
       "      <td>t1_cqug96h</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug96h</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>flal4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqudz0p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34f10p</td>\n",
       "      <td>t1_cqug9tk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cqug9tk</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HitachinoBia</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t1_cqufsip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "      <td>t1_cquga1l</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1l</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t3_34cvvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>t5_2qh3l</td>\n",
       "      <td>t3_34e7eo</td>\n",
       "      <td>t1_cquga1v</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>news</td>\n",
       "      <td>cquga1v</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Cultiststeve</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.432703e+09</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>t1_cqudxkr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc   ups subreddit_id    link_id        name  score_hidden  \\\n",
       "0  1430438402 -11.0     t5_2qh3l  t3_34f1lq  t1_cqug92b           0.0   \n",
       "1  1430438407   1.0     t5_2qh3l  t3_34exjb  t1_cqug96h           0.0   \n",
       "2  1430438439   4.0     t5_2qh3l  t3_34f10p  t1_cqug9tk           0.0   \n",
       "3  1430438448   0.0     t5_2qh3l  t3_34cvvg  t1_cquga1l           0.0   \n",
       "4  1430438449 -10.0     t5_2qh3l  t3_34e7eo  t1_cquga1v           0.0   \n",
       "\n",
       "   author_flair_css_class  author_flair_text subreddit       id  ...  downs  \\\n",
       "0                     NaN                NaN      news  cqug92b  ...    0.0   \n",
       "1                     NaN                NaN      news  cqug96h  ...    0.0   \n",
       "2                     NaN                NaN      news  cqug9tk  ...    0.0   \n",
       "3                     NaN                NaN      news  cquga1l  ...    0.0   \n",
       "4                     NaN                NaN      news  cquga1v  ...    0.0   \n",
       "\n",
       "   archived        author  score  retrieved_on  \\\n",
       "0       0.0     hogsucker  -11.0  1.432703e+09   \n",
       "1       0.0         flal4    1.0  1.432703e+09   \n",
       "2       0.0  HitachinoBia    4.0  1.432703e+09   \n",
       "3       0.0     [deleted]    0.0  1.432703e+09   \n",
       "4       0.0  Cultiststeve  -10.0  1.432703e+09   \n",
       "\n",
       "                                                body  distinguished edited  \\\n",
       "0  1-She got to be a bigwig at Google by sleeping...            NaN    0.0   \n",
       "1  For those about to lynch this guy [here](http:...            NaN    0.0   \n",
       "2  It feels like black people are the most racist...            NaN    0.0   \n",
       "3                                          [deleted]            NaN    0.0   \n",
       "4  Its because otherwise thats all that would app...            NaN    0.0   \n",
       "\n",
       "  controversiality   parent_id  \n",
       "0              0.0  t1_cqu4t11  \n",
       "1              1.0  t1_cqudz0p  \n",
       "2              1.0  t1_cqufsip  \n",
       "3              0.0   t3_34cvvg  \n",
       "4              0.0  t1_cqudxkr  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('../datasets/news-20k-comments.csv')\n",
    "politics_df = pd.read_csv(\"../datasets/politics-20k-comments.csv\")\n",
    "politicaldiscussion_df = pd.read_csv('../datasets/politicaldiscussion-20k-comments.csv')\n",
    "\n",
    "raw_df = pd.concat([news_df, politics_df, politicaldiscussion_df]) # merge all three subreddit datas\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = raw_df.filter(['created_utc', 'subreddit', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out deleted comments\n",
    "comments = all_comments[all_comments['body'] != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-993bcaba2f48>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['body'] = comments['body'].astype('str')\n",
      "<ipython-input-30-993bcaba2f48>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1 she got to be a bigwig at google by sleeping...\n",
       "1    for those about to lynch this guy here is a sh...\n",
       "2    it feels like black people are the most racist...\n",
       "4    its because otherwise thats all that would app...\n",
       "5    please go to facebook and comment and post on ...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean out any urls and and brackets, parenthesis and hyphens, leaving only alphanumeric words\n",
    "url_regex = r\"([--:\\w?@%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?\"\n",
    "special_character_regex = r\"[\\\"'()[\\]]\"\n",
    "\n",
    "comments['body'] = comments['body'].astype('str')\n",
    "\n",
    "#remove urls, special characters, and replace hyphens with a space\n",
    "comments['clean'] = comments['body'].apply(lambda text: text.strip().lower()).apply(lambda text: re.sub(url_regex, '', text)).apply(lambda text: re.sub(special_character_regex, '', text)).apply(lambda text: re.sub(r\"-\", ' ', text))\n",
    "comments['clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-d6433c9309aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)\n"
     ]
    }
   ],
   "source": [
    "comments['tokens'] = comments['clean'].apply(lambda text: re.sub(r\"[.,!?]\",\" \", text)).apply(lambda text: re.sub(r\"[0-9]\", \" \", text)).apply(nltk.wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55567 total comments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430438402</td>\n",
       "      <td>news</td>\n",
       "      <td>1-She got to be a bigwig at Google by sleeping...</td>\n",
       "      <td>1 she got to be a bigwig at google by sleeping...</td>\n",
       "      <td>[she, got, to, be, a, bigwig, at, google, by, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430438407</td>\n",
       "      <td>news</td>\n",
       "      <td>For those about to lynch this guy [here](http:...</td>\n",
       "      <td>for those about to lynch this guy here is a sh...</td>\n",
       "      <td>[for, those, about, to, lynch, this, guy, here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1430438439</td>\n",
       "      <td>news</td>\n",
       "      <td>It feels like black people are the most racist...</td>\n",
       "      <td>it feels like black people are the most racist...</td>\n",
       "      <td>[it, feels, like, black, people, are, the, mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1430438449</td>\n",
       "      <td>news</td>\n",
       "      <td>Its because otherwise thats all that would app...</td>\n",
       "      <td>its because otherwise thats all that would app...</td>\n",
       "      <td>[its, because, otherwise, thats, all, that, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1430438456</td>\n",
       "      <td>news</td>\n",
       "      <td>Please go to Facebook and comment and post on ...</td>\n",
       "      <td>please go to facebook and comment and post on ...</td>\n",
       "      <td>[please, go, to, facebook, and, comment, and, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  created_utc subreddit                                               body  \\\n",
       "0  1430438402      news  1-She got to be a bigwig at Google by sleeping...   \n",
       "1  1430438407      news  For those about to lynch this guy [here](http:...   \n",
       "2  1430438439      news  It feels like black people are the most racist...   \n",
       "3  1430438449      news  Its because otherwise thats all that would app...   \n",
       "4  1430438456      news  Please go to Facebook and comment and post on ...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  1 she got to be a bigwig at google by sleeping...   \n",
       "1  for those about to lynch this guy here is a sh...   \n",
       "2  it feels like black people are the most racist...   \n",
       "3  its because otherwise thats all that would app...   \n",
       "4  please go to facebook and comment and post on ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [she, got, to, be, a, bigwig, at, google, by, ...  \n",
       "1  [for, those, about, to, lynch, this, guy, here...  \n",
       "2  [it, feels, like, black, people, are, the, mos...  \n",
       "3  [its, because, otherwise, thats, all, that, wo...  \n",
       "4  [please, go, to, facebook, and, comment, and, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = comments.reset_index(drop=True)\n",
    "print(len(comments), \"total comments\")\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55567"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for comment in comments['clean'][:5000]:\n",
    "    all_text += \"<\" + comment + \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max id: 81 \n",
      "dataset_size: 1087981\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count\n",
    "print(\"max id:\", max_id, \"\\ndataset_size:\", dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([all_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NAME: RNN_GRU_128\n",
      "NOW: 20201209192256\n",
      "NAME: RNN_GRU_128-20201209192256\n",
      "ROOT LOGDIR: tf_logs/fit\n",
      "LOGDIR: tf_logs/fit/RNN_GRU_128-20201209192256/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_NAME = \"RNN_GRU_128\"\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "NAME = \"{}-{}\".format(MODEL_NAME, now)\n",
    "\n",
    "root_logdir = \"tf_logs/fit\"\n",
    "logdir = \"{}/{}-{}/\".format(root_logdir,MODEL_NAME, now)\n",
    "print(\"MODEL NAME:\", MODEL_NAME)\n",
    "print(\"NOW:\", now)\n",
    "print(\"NAME:\", NAME)\n",
    "print(\"ROOT LOGDIR:\",root_logdir)\n",
    "print(\"LOGDIR:\", logdir)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/RNN_GRU_128-20201209192256/'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = 'checkpoints/{}/'.format(NAME)\n",
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_acc'),\n",
    "    keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN_GRU_128-20201209192256\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, None, 128)         81024     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 81)          10449     \n",
      "=================================================================\n",
      "Total params: 190,545\n",
      "Trainable params: 190,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_id))\n",
    "], name=NAME)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30597/30597 [==============================] - 439s 14ms/step - loss: 2.8348\n",
      "Epoch 2/10\n",
      "16628/30597 [===============>..............] - ETA: 3:18 - loss: 2.7176"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir tf_logs/fit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
